---
title: Ilya Sutskever ML Study Guide
subtitle: A collection of papers recommended by Ilya to get a foundation in ML.
date: "2024-11-27"
categories: [recommendations]
image: /assets/img/ai_tech_tree.png
---

*This post lists Ilya Ilya Sutskever's ML study guide. For my own ML study guide, see [this post](https://richarddecal.com/posts/2024-01-17-Richards-ML-Study-Guide.html)*.

When Ilya gave this list to [John Carmack](https://x.com/ID_AA_Carmack) he reportedly said, ‘If you really learn all of these, you’ll know 90% of what matters today.’ 

There are a few versions of this list online, but the most authoratative I found was from Andrew Carr, an ex-OpenAI employee who found the list in OpenAI's on-boarding docs ([source](https://x.com/andrew_n_carr/status/1752526711311507526)). The most notable difference with this list is the source for Komogorov complexity, which is a chapter out of a book.


- **The Annotated Transformer.** Sasha Rush, et al. Companion article to "Attention is All You Need". [[Blog]](https://nlp.seas.harvard.edu/annotated-transformer/) [[GitHub]](https://github.com/harvardnlp/annotated-transformer/)
- **Attention Is All You Need.** Ashish Vaswani, et al. [[ArXiv]](https://arxiv.org/abs/1706.03762.pdf)
- **The First Law of Complexodynamics.** Scott Aaronson. [[Blog]](https://scottaaronson.blog/?p=762)
- **The Unreasonable Effectiveness of Recurrent Neural Networks.** Andrej Karpathy. [[Blog]](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)
- **Understanding LSTM Networks.** Christopher Olah. [[Blog]](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- **Recurrent Neural Network Regularization.** Wojciech Zaremba, et al. [[ArXiv]](https://arxiv.org/abs/1409.2329)
- **Keeping Neural Networks Simple by Minimizing the Description Length of the Weights.** Geoffrey E. Hinton and Drew van Camp. [[PDF]](https://www.cs.toronto.edu/~hinton/absps/colt93.pdf)
- **Pointer Networks.** Oriol Vinyals, et al. [[ArXiv]](https://arxiv.org/abs/1506.03134.pdf)
- **ImageNet Classification with Deep Convolutional Neural Networks.** Alex Krizhevsky, et al. [[PDF]](https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)
- **Order Matters: Sequence to sequence for sets.** Oriol Vinyals, et al. [[ArXiv]](https://arxiv.org/abs/1511.06391.pdf)
- **GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism.** Yanping Huang, et al. [[ArXiv]](https://arxiv.org/abs/1811.06965.pdf)
- **Deep Residual Learning for Image Recognition.** Kaiming He, et al. [[ArXiv]](https://arxiv.org/abs/1512.03385.pdf)
- **Multi-Scale Context Aggregation by Dilated Convolutions.** Fisher Yu and Vladlen Koltun. [[ArXiv]](https://arxiv.org/abs/1511.07122.pdf)
- **Neural Message Passing for Quantum Chemistry.** Justin Gilmer, et al. [[ArXiv]](https://arxiv.org/abs/1704.01212.pdf)
- **Attention Is All You Need.** Ashish Vaswani, et al. [[ArXiv]](https://arxiv.org/abs/1706.03762.pdf)
- **Neural Machine Translation by Jointly Learning to Align and Translate.** Dzmitry Bahdanau, et al. [[ArXiv]](https://arxiv.org/abs/1409.0473.pdf)
- **Identity Mappings in Deep Residual Networks.** Kaiming He, et al. [[ArXiv]](https://arxiv.org/abs/1603.05027.pdf)
- **A simple neural network module for relational reasoning.** Adam Santoro, et al. [[ArXiv]](https://arxiv.org/abs/1706.01427.pdf)
- **Variational Lossy Autoencoder.** Xi Chen, et al. [[ArXiv]](https://arxiv.org/abs/1611.02731.pdf)
- **Relational recurrent neural networks.** Adam Santoro, et al. [[ArXiv]](https://arxiv.org/abs/1806.01822.pdf)
- **Quantifying the Rise and Fall of Complexity in Closed Systems: The Coffee Automaton.** Scott Aaronson, et al. [[ArXiv]](https://arxiv.org/abs/1405.6903.pdf)
- **Neural Turing Machines.** Alex Graves, et al. [[ArXiv]](https://arxiv.org/abs/1410.5401.pdf)
- **Deep Speech 2: End-to-End Speech Recognition in English and Mandarin.** Dario Amodei, et al. [[ArXiv]](https://arxiv.org/abs/1512.02595.pdf)
- **Scaling Laws for Neural Language Models.** Jared Kaplan, et al. [[ArXiv]](https://arxiv.org/abs/2001.08361.pdf)
- **A Tutorial Introduction to the Minimum Description Length Principle.** Peter Grunwald. [[ArXiv]](https://arxiv.org/abs/math/0406077.pdf)
- **Machine Super Intelligence.** Shane Legg. [[Thesis PDF]](https://www.vetta.org/documents/Machine_Super_Intelligence.pdf)
- **CS231n: Convolutional Neural Networks for Visual Recognition.** [[Course]](https://cs231n.github.io/)
* **Elements of Information Theory, 2nd Edition, Ch. 14 Komogorov Complexity**. Thomas M. Cover, Joy A. Thomas. [[Book]](https://www.amazon.com/Elements-Information-Theory-Telecommunications-Processing/dp/0471241954)

Note that there are additional papers on meta-learning that are missing. If anyone finds them, *please let me know so that I can add them to the list*. In the meantime, I will suggest two meta-learning papers here:

* **Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments.** Al-Shedivat et al. [[ArXiv]](https://arxiv.org/abs/1710.03641)
* **Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.** Chelsea Finn, Pieter Abbeel, Sergey Levine. [[Project page]](https://proceedings.mlr.press/v70/finn17a.html)