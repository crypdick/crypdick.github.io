---
title: Ilya Sutskever ML Study Guide
subtitle: A collection of papers recommended by Ilya to get a foundation in ML.
date: "2024-11-27"
categories: [recommendations]
image: /assets/img/ai_tech_tree.png
---

*This post lists Ilya Ilya Sutskever's ML study guide. For my own ML study guide, see [this post](https://richarddecal.com/posts/2024-01-17-Richards-ML-Study-Guide.html)*.

When Ilya gave this list to [John Carmack](https://x.com/ID_AA_Carmack) he reportedly said, ‘If you really learn all of these, you’ll know 90% of what matters today.’ There are a few versions of this list online, but the most authoratative I found was from Andrew Carr, an ex-OpenAI employee who found the list in OpenAI's on-boarding docs ([source](https://x.com/andrew_n_carr/status/1752526711311507526)).


This list is also available as an ARC folder [here](https://arc.net/folder/D0472A20-9C20-4D3F-B145-D2865C0A9FEE). However, the paper on Komogorov complexity is not the one that Ilya recommended.


- **The Annotated Transformer.** Sasha Rush, et al. [[Blog]](https://nlp.seas.harvard.edu/annotated-transformer/) [[GitHub]](https://github.com/harvardnlp/annotated-transformer/)
- **The First Law of Complexodynamics.** Scott Aaronson. [[Blog]](https://scottaaronson.blog/?p=762)
- **The Unreasonable Effectiveness of Recurrent Neural Networks.** Andrej Karpathy. [[Blog]](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)
- **Understanding LSTM Networks.** Christopher Olah. [[Blog]](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- **Recurrent Neural Network Regularization.** Wojciech Zaremba, et al. [[ArXiv]](https://arxiv.org/abs/1409.2329) [[ArXiv]](https://arxiv.org/abs/1409.2329)
- **Keeping Neural Networks Simple by Minimizing the Description Length of the Weights.** Geoffrey E. Hinton and Drew van Camp.
- **Pointer Networks.** Oriol Vinyals, et al.
- **ImageNet Classification with Deep Convolutional Neural Networks.** Alex Krizhevsky, et al.
- **Order Matters: Sequence to sequence for sets.** Oriol Vinyals, et al.
- **GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism.** Yanping Huang, et al.
- **Deep Residual Learning for Image Recognition.** Kaiming He, et al.
- **Multi-Scale Context Aggregation by Dilated Convolutions.** Fisher Yu and Vladlen Koltun.
- **Neural Message Passing for Quantum Chemistry.** Justin Gilmer, et al.
- **Attention Is All You Need.** Ashish Vaswani, et al.
- **Neural Machine Translation by Jointly Learning to Align and Translate.** Dzmitry Bahdanau, et al.
- **Identity Mappings in Deep Residual Networks.** Kaiming He, et al.
- **A simple neural network module for relational reasoning.** Adam Santoro, et al.
- **Variational Lossy Autoencoder.** Xi Chen, et al.
- **Relational recurrent neural networks.** Adam Santoro, et al.
- **Quantifying the Rise and Fall of Complexity in Closed Systems: The Coffee Automaton.** Scott Aaronson, et al.
- **Neural Turing Machines.** Alex Graves, et al.
- **Deep Speech 2: End-to-End Speech Recognition in English and Mandarin.** Dario Amodei, et al.
- **Scaling Laws for Neural Language Models.** Jared Kaplan, et al.
- **A Tutorial Introduction to the Minimum Description Length Principle.** Peter Grunwald.
- **Machine Super Intelligence.** Shane Legg.
- **CS231n: Convolutional Neural Networks for Visual Recognition.**
* **Elements of Information Theory, 2nd Edition, Ch. 14 Komogorov Complexity**. Thomas M. Cover, Joy A. Thomas.

Note that there are additional papers on meta-learning that are missing. If anyone finds them, *please let me know so that I can add them to the list*. In the meantime, I will suggest two meta-learning papers here:

* **Continuous Adaptation via Meta-Learning in Nonstationary and Competitive Environments.** Al-Shedivat et al. [[ArXiv]](https://arxiv.org/abs/1710.03641)
* **Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks.** Chelsea Finn, Pieter Abbeel, Sergey Levine. [[Project page]](https://proceedings.mlr.press/v70/finn17a.html)